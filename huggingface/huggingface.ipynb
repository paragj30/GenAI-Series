{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='00 - Employee Handbook\\nOwnerLaura Kuhlmann \\xa0use @mention\\nLast review07 May 2024 \\xa0 use date picker\\nStatus PUBLISHED  |  |  | DraftWiP In Review Published | Deprecated\\nConfidentiality DCS INTERNAL Public | DCS Internal | DCS and partners | Confidential | Strictly \\nconfidential,\\nblocked \\nURLThe Employee Handbook provides an\\xa0 , especially the orientation to all employees\\nnewbies!\\xa0\\nIt provides a short overview of important information regarding\\xa0 \\xa0and\\xa0 people topics b\\n. ehavioral guidelines\\nThe Handbook is\\xa0 \\xa0structured and every subject is divided into a description glossary\\nof the general framework,\\nthe process and the need-to-know with further links and the responsible persons.\\xa0\\n for further suggestions on topics to be covered, please contact the People & Culture team!\\nAB C D E F\\nAb\\nbre\\nvia\\ntio\\nns\\nAc\\nce\\nss \\nCa\\nrdBAV-\\nAllow\\nance\\nBudd\\ny \\nConc\\nept\\nBEV \\nProgr\\nam \\n(FINN)Certificate of \\nEmployment / \\nReference \\nLetter\\nCandidate \\nJourney\\nCode of ethics\\nCompensation \\n& Benefits\\nCompliance', metadata={'source': 'us_census\\\\acsbr-015.pdf', 'page': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the ppdfs from the folder\n",
    "loader=PyPDFDirectoryLoader(\"./us_census\")\n",
    "\n",
    "documents=loader.load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "\n",
    "final_documents=text_splitter.split_documents(documents)\n",
    "final_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ParagJadhav\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Embedding Using Huggingface\n",
    "huggingface_embeddings=HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",      #sentence-transformers/all-MiniLM-l6-v2\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True}\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.59445831e-02  5.15468307e-02 -9.89440177e-03  1.50767984e-02\n",
      "  6.52042106e-02  4.49973717e-02  6.33552223e-02  1.87990107e-02\n",
      " -3.14946845e-02  5.63665256e-02  2.84940880e-02  2.33569113e-03\n",
      "  1.36262327e-02 -3.35915796e-02 -1.67123489e-02  2.44941693e-02\n",
      "  1.33883441e-02 -3.81309427e-02  2.39970554e-02 -6.28772285e-03\n",
      "  3.80139425e-02 -2.12840401e-02 -2.16505062e-02  2.90671363e-02\n",
      " -2.63215564e-02  3.65823088e-03 -4.29292023e-02  7.27340672e-03\n",
      " -3.07316463e-02 -9.53067467e-02  8.05057213e-03 -1.59666408e-02\n",
      " -2.03691814e-02  2.92067174e-02  3.01305484e-03  8.72481894e-03\n",
      "  1.89054459e-02  3.01838946e-03  6.58440515e-02  4.51830626e-02\n",
      " -9.73373875e-02  2.05465183e-02  5.04652271e-03 -2.64394190e-02\n",
      "  1.66579504e-02  6.47821138e-03 -4.64136153e-03  1.99217699e-03\n",
      " -4.77383547e-02 -3.39184105e-02 -4.71404195e-03 -2.70152409e-02\n",
      "  1.67670362e-02  2.11288389e-02  2.43268516e-02 -2.28119195e-02\n",
      "  1.06723055e-01  3.91150303e-02 -5.08910511e-03  2.55724341e-02\n",
      "  5.77243343e-02 -2.76102442e-02 -1.76328957e-01  1.12851560e-01\n",
      " -1.65230352e-02  3.85929234e-02 -2.02658940e-02 -8.50540958e-03\n",
      " -3.44525911e-02 -6.71748444e-02  4.27456014e-02  7.64396391e-04\n",
      " -5.06833242e-03  1.99863221e-02 -4.95160967e-02  2.81591844e-02\n",
      " -1.27071887e-02  3.55328508e-02 -6.21780613e-03  3.27116176e-02\n",
      " -7.30613917e-02  4.60174419e-02  3.29031311e-02 -1.97555572e-02\n",
      " -1.14122517e-02  6.80349991e-02  4.22671326e-02 -1.80823430e-02\n",
      "  4.43743877e-02 -2.21176315e-02  8.46106037e-02 -4.17027436e-02\n",
      "  1.49447592e-02 -5.10307886e-02 -3.40432562e-02 -8.99542943e-02\n",
      "  1.54891126e-02  1.27007840e-02 -4.40989695e-02  3.63938421e-01\n",
      " -2.70969272e-02  1.10526793e-02  2.77459435e-03  1.25666130e-02\n",
      "  3.49994861e-02  1.62970070e-02  6.20390009e-03 -9.49836336e-03\n",
      "  7.13122217e-03  1.54819069e-02  1.84407346e-02  6.93062693e-02\n",
      " -8.99482111e-05 -6.36232272e-02  6.33460656e-02  2.01304797e-02\n",
      "  1.04765221e-02  5.83106428e-02  6.64332435e-02  2.75516342e-02\n",
      "  7.99625553e-03 -1.02862250e-03  5.38985385e-03  3.88259366e-02\n",
      "  2.17946265e-02  2.85556316e-02  1.85061507e-02  1.80751700e-02\n",
      "  4.63109687e-02  1.94379129e-02  6.68041334e-02  5.60957976e-02\n",
      " -2.81320252e-02  6.69404399e-04  6.51659071e-03  2.07679886e-02\n",
      "  1.36890309e-03 -5.81017742e-03  6.29538372e-02 -1.74127519e-02\n",
      "  4.09966148e-03 -4.78461087e-02  3.79098877e-02 -5.15077971e-02\n",
      " -6.29802570e-02  1.15465283e-01 -5.58183379e-02  2.23398749e-02\n",
      " -1.12718698e-02  4.59139496e-02  1.70404073e-02  8.00893903e-02\n",
      " -6.72207177e-02 -6.47105202e-02  1.50273861e-02 -3.70735079e-02\n",
      "  8.24512076e-03  4.93161269e-02 -1.19124623e-02 -4.80918810e-02\n",
      " -4.20671655e-03 -1.08972050e-01 -6.88712895e-02  6.89300522e-02\n",
      " -3.95316780e-02 -1.19374081e-01  8.33461818e-05  4.29530926e-02\n",
      "  5.32310754e-02 -2.46277973e-02 -2.62550097e-02 -2.60116197e-02\n",
      " -5.99719808e-02  4.95281555e-02 -1.17389280e-02  1.86458193e-02\n",
      "  6.89015910e-02 -4.08711005e-03 -4.28115427e-02  3.40391546e-02\n",
      "  2.86593810e-02  9.42294486e-03 -1.66549385e-02 -2.12924811e-03\n",
      "  3.01622357e-02 -1.88245773e-02 -3.40171307e-02 -2.71146856e-02\n",
      "  3.37429792e-02  1.00013074e-02 -6.80150539e-02 -2.02105716e-02\n",
      " -3.67270485e-02  1.75742041e-02 -1.19871609e-02  3.54393497e-02\n",
      " -1.38398930e-01 -8.78012273e-03 -2.94033606e-02  2.88182986e-03\n",
      " -1.78330150e-02  8.48232880e-02 -6.29967591e-03  3.78277041e-02\n",
      " -1.01432940e-02  3.07094138e-02  7.45622069e-02 -5.31264730e-02\n",
      "  5.87846227e-02 -1.54732456e-02 -8.46354440e-02  2.85841376e-02\n",
      " -1.62736494e-02 -6.12125779e-03  3.08363326e-02  3.85356843e-02\n",
      "  1.02852724e-01  4.72002849e-02  1.07927984e-02 -3.42528187e-02\n",
      "  1.26302429e-02  6.56420784e-03 -4.23508091e-03 -3.31953019e-01\n",
      "  3.47199440e-02 -6.11971430e-02 -8.14302650e-04 -1.96663905e-02\n",
      "  6.18583187e-02  2.98512466e-02 -3.81596871e-02 -2.15150472e-02\n",
      "  1.16756689e-02  1.34641960e-01  2.08367910e-02 -2.56026927e-02\n",
      " -2.33893674e-02  2.27603521e-02  4.84178662e-02 -1.93230864e-02\n",
      "  6.75226701e-03 -7.47282952e-02  4.58470499e-03  1.30692106e-02\n",
      " -1.56817865e-02 -5.58204083e-05 -2.90423017e-02  7.42895603e-02\n",
      " -3.96411903e-02  7.04999417e-02 -8.38829428e-02 -3.40536870e-02\n",
      " -5.77335097e-02 -2.72931457e-02  2.06879936e-02 -1.40488865e-02\n",
      " -1.40274629e-01  2.95734834e-02 -2.60066576e-02 -3.49005274e-02\n",
      "  2.18664715e-03 -1.66126899e-02 -6.05709180e-02  2.93749943e-02\n",
      "  2.59190071e-02 -2.09219642e-02 -1.25549780e-03 -2.90935934e-02\n",
      " -4.06566039e-02 -3.18174772e-02 -2.07936186e-02 -1.78858694e-02\n",
      " -1.48043716e-02  3.87565419e-02  7.76305096e-04 -1.62624859e-03\n",
      "  3.89633626e-02 -4.90891002e-02 -3.69777083e-02 -7.00632483e-03\n",
      " -2.33286582e-02 -4.75457273e-02  5.65704517e-03 -2.74726506e-02\n",
      "  6.82105660e-04  2.10680310e-02 -7.56949000e-03 -2.98453514e-02\n",
      " -5.33745736e-02 -2.22372413e-02  3.08639333e-02  2.89699771e-02\n",
      "  3.22790537e-03 -1.99579708e-02  4.77866195e-02 -1.70620345e-02\n",
      " -6.74716150e-03 -2.30411775e-02 -4.11679549e-03 -1.06289824e-02\n",
      " -1.31778931e-02 -1.46826617e-02 -1.55739654e-02 -5.86102232e-02\n",
      " -4.68207076e-02 -2.54237968e-02  2.97667794e-02  3.01734898e-02\n",
      "  1.84245110e-02  2.81201620e-02 -5.72369099e-02  9.83805433e-02\n",
      "  2.48705987e-02  5.35811414e-04  3.47398855e-02 -7.94939697e-03\n",
      " -2.10863687e-02  3.78124565e-02 -5.30692143e-03 -2.50573307e-01\n",
      "  7.14111002e-03 -6.22640327e-02  4.63170782e-02  5.44193685e-02\n",
      "  3.27168591e-03 -2.42787953e-02 -6.48818240e-02 -6.01940267e-02\n",
      " -2.45613977e-02  5.89437522e-02  1.18313879e-02 -7.87279755e-03\n",
      " -4.14212160e-02  7.88331125e-03  3.99453864e-02  8.83091539e-02\n",
      " -1.50585258e-02  1.76625128e-03  9.50883422e-03 -2.09984239e-04\n",
      " -5.70812300e-02  1.37052074e-01  2.58543398e-02  5.16374595e-04\n",
      " -2.33946275e-02 -3.95807344e-03  4.05924879e-02 -2.05346812e-02\n",
      "  1.44125437e-02  3.06964964e-02 -2.83792447e-02  1.05879724e-01\n",
      "  4.61093374e-02 -4.50947061e-02 -2.66083535e-02 -4.77508344e-02\n",
      "  7.94872493e-02  2.80253422e-02 -4.55568731e-03 -2.63514481e-02\n",
      " -9.71555524e-03  3.66182812e-02 -2.24838685e-02  2.80854516e-02\n",
      "  8.46365374e-03 -8.63473713e-02 -6.29659966e-02  1.91942304e-02\n",
      " -4.84340079e-02 -3.41078639e-02 -5.96071519e-02  2.68774088e-02\n",
      "  5.80743030e-02  3.02031040e-02  6.71089161e-03  4.17729281e-02\n",
      " -6.70194104e-02 -3.73459794e-02 -3.37938033e-03 -4.47822735e-03\n",
      "  6.26482219e-02 -4.15241485e-03  2.58002505e-02  5.18993475e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import  numpy as np\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)))\n",
    "print(np.array(huggingface_embeddings.embed_query(final_documents[0].page_content)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorStore Creation\n",
    "vectorstore=FAISS.from_documents(final_documents,huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "membership is directly linked to your membership.\n",
      "Process 1. Go to the EGYM Wellpass lading page\n",
      "2. Register till 20th of a month\n",
      "3. Start your training on 1st day of the following month\n",
      "Need-\n",
      "to-knowfor further information please see 05-II. EGYM Wellpass - Fitness Cooperation\n",
      "Your contact person in this matter is Laura \n",
      "Gift & Invitation\n",
      "Frame You can find the gift & invitation policy here: 20-Processes, policies and \n",
      "compliance\n",
      "Need-to-\n",
      "know\n",
      "Grading - under construction\n",
      "Frame New Growth Framework launched in July 2022\n",
      "Process Get more information here:  04 - New Growth Framework (NGF) OLD\n",
      "Need-to-know Your contact person in this matter is Kai\n",
      "****************************************************************************************************\n",
      "FrameAt DCS, we want to foster a feedback culture as from time to time it is good to \n",
      "synchronize views and perspectives. Generally it is recommended to exchange on a \n",
      "regular basis.\n",
      "Proce\n",
      "ssEspecially for Newbies, feedback helps to align his/her \"as is world\" to the new \n",
      "environment and expectations\n",
      "Newbies ask for it and put it in the calendar of the line manager and peers \n",
      "Need-\n",
      "to-\n",
      "knowFor more information please see Feedback within the employee lifecycle\n",
      "Your contact person in this matter is Kai \n",
      "Fitness Cooperation  EGYM Wellpass (former Qualitrain)\n",
      "Frame All employees (except externals) can become member of EGYM Wellpass at special \n",
      "DCS conditions.\n",
      "You can add a friend as \"plus1\", who doesn't work at our company. The partner's \n",
      "membership is directly linked to your membership.\n",
      "Process 1. Go to the EGYM Wellpass lading page\n",
      "2. Register till 20th of a month\n",
      "3. Start your training on 1st day of the following month\n",
      "Need-\n"
     ]
    }
   ],
   "source": [
    "## Query using Similarity Search\n",
    "query=\"what is EGYM Wellpass?\"\n",
    "relevant_docments=vectorstore.similarity_search(query)\n",
    "\n",
    "print(relevant_docments[0].page_content)\n",
    "print('*'*100)\n",
    "print(relevant_docments[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'HuggingFaceBgeEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021089C87D60> search_kwargs={'k': 3}\n"
     ]
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(search_type=\"similarity\",search_kwargs={\"k\":3})\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN']=\"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\"\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# os.environ['OPENAI_API_KEY']=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: iYhfTGgYE1-YZzjHFs-2k)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m\n\u001b[0;32m      3\u001b[0m hf\u001b[38;5;241m=\u001b[39mHuggingFaceHub(\n\u001b[0;32m      4\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     model_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m500\u001b[39m}\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is EGYM Wellpass?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mhf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1317\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1314\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1316\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1320\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\langchain_community\\llms\\huggingface_hub.py:135\u001b[0m, in \u001b[0;36mHuggingFaceHub._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m _model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    133\u001b[0m parameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_model_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 135\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:273\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\ParagJadhav\\Downloads\\LLM\\Langchain\\1- Lets Learn About Langchain-What We Will Learn And Demo Projects\\day1\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-7B-v0.1 (Request ID: iYhfTGgYE1-YZzjHFs-2k)\n\nRate limit reached. You reached free usage limit (reset hourly). Please subscribe to a plan at https://huggingface.co/pricing to use the API at this rate"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    model_kwargs={\"temperature\":0.1,\"max_length\":500}\n",
    "\n",
    ")\n",
    "query=\"what is EGYM Wellpass?\"\n",
    "hf.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hugging Face models can be run locally through the HuggingFacePipeline class.\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"temperature\": 0, \"max_new_tokens\": 300}\n",
    ")\n",
    "\n",
    "llm = hf \n",
    "llm.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following piece of context to answer the question asked.\n",
    "Please try to provide the answer only based on the context\n",
    "\n",
    "{context}\n",
    "Question:{question}\n",
    "\n",
    "Helpful Answers:\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievalQA=RetrievalQA.from_chain_type(\n",
    "    llm=hf,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\":prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"DIFFERENCES IN THE\n",
    "UNINSURED RATE BY STATE\n",
    "IN 2022\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the QA chain with our query.\n",
    "result = retrievalQA.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
